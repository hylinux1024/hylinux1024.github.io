<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>机器学习之逻辑回归 &#8211; Machine Learning</title>
<meta name="description" content="逻辑回归（Logistic Regression）是机器学习中的监督学习算法，主要用于分类任务。这些任务预测的标签（label）通常为{0,1}，或者说是一个概率值。 例如我们预测一封邮件是否为垃圾邮件；预测天气是否为晴天、阴天、雨天和雪天等。">
<meta name="keywords" content="逻辑回归, Logistic Regression">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之逻辑回归">
<meta property="og:description" content="逻辑回归（Logistic Regression）是机器学习中的监督学习算法，主要用于分类任务。这些任务预测的标签（label）通常为{0,1}，或者说是一个概率值。 例如我们预测一封邮件是否为垃圾邮件；预测天气是否为晴天、阴天、雨天和雪天等。">
<meta property="og:url" content="/logisitic-regression/">
<meta property="og:site_name" content="Machine Learning">





<link rel="canonical" href="/logisitic-regression/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Machine Learning Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/avatar.jpg" alt="wecodexyz photo" class="author-photo">
					<h4>wecodexyz</h4>
					<p>机器学习爱好者</p>
				</li>
				<li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:wecodexyz@email.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				
				<li>
					<a href="https://github.com/https://github.com/wecodexyz"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/tags/">All Tags</a></li>
			</ul>
		</li>
		
	    
	    <li><a href="/theme-setup/" >Theme Setup</a></li>
	  
	    
	    <li><a href="http://mademistakes.com" target="_blank">External Link</a></li>
	  
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="/logisitic-regression/" rel="bookmark" title="机器学习之逻辑回归">机器学习之逻辑回归</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2017-08-20T00:00:00+08:00">August 20, 2017</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
Reading time ~1 minute
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>本文主要内容</p>

<ul>
  <li>逻辑回归（Logistic Regression）模型</li>
  <li>决策边界（Decision Boundary）</li>
  <li>成本函数（Cost Function）</li>
  <li>梯度下降法（Gradient Descent）</li>
  <li>多分类问题</li>
</ul>

<p>逻辑回归（Logistic Regression）是机器学习中的监督学习算法，主要用于分类任务。这些任务预测的标签（label）通常为 {0,1}，或者说是一个概率值。 例如我们预测一封邮件是否为垃圾邮件；预测天气是否为晴天、阴天、雨天和雪天等。</p>

<h3 id="logistic-regression">逻辑回归（Logistic Regression）模型</h3>

<p>从上文《机器学习之线性回归》我们知道线性回归的模型为
<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x_1+...+\theta_nx_n=\theta^Tx</script>
可以从上面模型通过函数 g 对线性回归进行变换
<script type="math/tex">h_\theta(x)=g(\theta^Tx) \tag 1</script>
其中
<script type="math/tex">g(z)=\frac 1 {1+e^{-z}} \tag 2</script>
其中g(z) 是 <strong>sigmoid 函数或者叫逻辑函数</strong>。可以看出 0&lt;= g(z)&lt;=1</p>

<p><img src="http://angrycode.qiniudn.com/sigmoid_functio.jpeg" alt="sigmoid function" /></p>

<p>由 (1) 和 (2)
<script type="math/tex">h_\theta(x)=\frac 1 {1+e^{-\theta^Tx}} \tag 3</script>
这就是<strong>逻辑回归的数学模型</strong>。</p>

<p>因为预测值 h(x) 是在{0,1}，故还可以用概率的角度进行解析这个模型的预测结果。</p>

<p>以预测垃圾邮件为例，如果 y=1 表示这封邮件时垃圾邮件，那么预测模型可以表示
<script type="math/tex">h_\theta(x)=P(y=1|x;\theta) \tag 4</script></p>

<h3 id="decision-boundary">决策边界（Decision Boundary）</h3>

<p>我们知道 sigmoid 函数的值是在 {0,1} 范围，当 \(h_\theta(x)\geq0.5\) 时，预测y=1，反之则预测 y=0。</p>

<p>根据 sigmoid 函数可以知道要预测 \(h_\theta(x)\geq0.5\) 那么等价于 \(\theta^Tx\geq0\)</p>

<p>我们以这张来自于 Andrew Ng 机器学习课程的 ppt 来说明。</p>

<p><img src="http://angrycode.qiniudn.com/decision_boundary.png" alt="" /></p>

<p>假设通过训练得出
<script type="math/tex">\theta=\begin{bmatrix}
-3\\
1\\
1
\end{bmatrix}</script>
那么
<script type="math/tex">x_1+x_2=3 \tag 5</script>
就是<strong>决策边界</strong>。当
<script type="math/tex">x_1+x_2\geq3</script>
表示预测值 y=1，反之预测值 y=0。</p>

<p>除了线性决策边界，还可以是曲线的。</p>

<p><img src="http://angrycode.qiniudn.com/non-linear-decision-boundary.png" alt="non-linear-decision-boundary" /></p>

<p>这里的决策边界就是
<script type="math/tex">x_1+x_2=1 \tag 6</script>
以上两个例子都是简单的边界，当然实际情况下，决策边界有能是复杂的图形，甚至是高维度的。</p>

<h3 id="cost-function">成本函数（Cost Function）</h3>

<p>回忆一下线性回归中的成本函数
<script type="math/tex">J(\theta)=\frac 1 {2m} \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script>
我们定义函数
<script type="math/tex">Cost(h_\theta(x),y)=\frac 1 2 (h_\theta(x)-y)^2</script>
那么成本函数就可以写成
<script type="math/tex">J(\theta)=\frac 1 m \sum_{i=1}^mCost(h_\theta(x),y)</script>
<strong>逻辑回归的成本函数（Cost Function）</strong>
<script type="math/tex">% <![CDATA[
Cost(h_\theta(x),y)=
\begin{cases}
-log(h_\theta(x)) & \text{if $y$ =1} \\
-log(1-h_\theta(x)) & \text{if $y$ =0}
\end{cases} \tag 7 %]]></script>
当 y=1 时</p>

<p><img src="http://angrycode.qiniudn.com/minus-logx.png?imageView2/2/w/200/h/200" alt="" /></p>

<p>当 y=0 时</p>

<p><img src="http://angrycode.qiniudn.com/minus-log1-x.png?imageView2/2/w/200/h/200" alt="" /></p>

<p>由于 y 取值是{0,1}，我们可以把公式 (7) 写成
<script type="math/tex">Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x)) \\ y\in {\{0,1\}}\tag 8</script></p>

<h3 id="gradient-descent">梯度下降法（Gradient Descent）</h3>

<p>由上文可以知道成本函数为
<script type="math/tex">J(\theta)=-\frac 1 m \sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))</script>
要求\(min_{\theta}J(\theta)\)，需要使用微分求导公式
<script type="math/tex">f(x)'=(log_ax)'=\frac 1 x lna</script>
同时为了简单起见，求导时可以假设只有一个样本，这样可以把∑符号去掉，方便推导。</p>

<p>因为上文中 log 函数默认是以 e 底的
<script type="math/tex">log(h_\theta(x))' =\frac 1 {h_\theta(x)} \frac d {d\theta}h_\theta(x)</script>
因为
<script type="math/tex">g(z)=\frac 1 {1+e^{-z}}</script>
有
<script type="math/tex">% <![CDATA[
\begin{equation}\begin{split}g(z)'&=\frac d {dz}(1+e^{-z})^{-1} \\
&=-(1+e^{-z})^{-2} \cdot (e^{-z})\cdot(-1)\\
&=\frac {e^{-z}} {(1+e^{-z})^2}\\
&=g(z) \cdot (\frac {e^{-z}} {1+e^{-z}})\\
&=g(z)(1-g(z))
\end{split}\end{equation} %]]></script>
那么有
<script type="math/tex">\frac d {d\theta_j}g(\theta^Tx)=g(\theta^Tx)(1-g(\theta^Tx))\theta_j</script>
于是
<script type="math/tex">J(\theta_j)'=\frac 1 m \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j \\
j \in {\{0,1,2...n\}}
\tag 9</script>
这个式子形式上与线性回归的\(J(\theta)\)的偏导数是很相似的，但是需要注意的是
\
预测函数 h(x) 是不一样的。</p>

<p>根据梯度下降法
<script type="math/tex">% <![CDATA[
\begin{equation}\begin{split}repeat &\{\\
&\theta_j := \theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j \\
\}
\end{split}\end{equation} %]]></script></p>

<h3 id="section">多分类问题</h3>

<p>对于二分类问题，我们很容易就可以建立模型。如果预测一个多分类问题，我们应该如何做呢？我们还是以Andrew Ng的ppt来说明</p>

<p><img src="http://angrycode.qiniudn.com/multi-class.png" alt="multi-class" /></p>

<p>例如要预测一个三分类的问题，假设要预测class1，那么就可以其他两类class2，class3当做一个分类，这样转换成一个二分类问题。这时候我们就需要根据已有的训练数据构造新的训练数据。预测class2，class3时，以此类推。这样就需要有三个不同训练数据。</p>

<p>当有一个新的数据进行预测时，我们只要计算下面式子的最大值就可以了
<script type="math/tex">\max_i h_\theta(x^{(i)})\\ i\in\{1,2,3\}</script></p>

<h3 id="section-1">参考文献</h3>

<p>Andrew Ng coursera 机器学习课程</p>


      <footer class="entry-meta">
        <span class="entry-tags"><a href="/tags/#逻辑回归" title="Pages tagged 逻辑回归" class="tag"><span class="term">逻辑回归</span></a><a href="/tags/#Logistic Regression" title="Pages tagged Logistic Regression" class="tag"><span class="term">Logistic Regression</span></a></span>
        <span>Updated on <span class="entry-date date updated"><time datetime="2017-08-20">August 20, 2017</time></span></span>
        <span class="author vcard"><span class="fn">wecodexyz</span></span>
        
      </footer>
    </div><!-- /.entry-content -->
    
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="/naive-bayesian/" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="/naive-bayesian/" title="机器学习之朴素贝叶斯">机器学习之朴素贝叶斯</a></h3>
      <p>贝叶斯定理又称贝叶斯法则。 <a href="/naive-bayesian/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="/linear-regression/" title="机器学习之线性回归">机器学习之线性回归</a></h4>
        <span>Published on August 07, 2017</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2017 wecodexyz. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	        

</body>
</html>
