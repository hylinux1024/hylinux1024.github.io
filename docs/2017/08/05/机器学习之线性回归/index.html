<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>机器学习之线性回归 | Machine Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="线性回归属于监督学习算法中的一种，也是最简单的一种算法。本文将讲述以下内容  Linear Regression 线性回归 Cost Function 成本函数 Gradient Decent 梯度下降法 Normal Equations 正规方程 Probabilistic interpretation 概率论解释  Linear Regression 线性回归简单地$$h_\theta(x)=">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之线性回归">
<meta property="og:url" content="http://wecodexyz.github.io/2017/08/05/机器学习之线性回归/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="线性回归属于监督学习算法中的一种，也是最简单的一种算法。本文将讲述以下内容  Linear Regression 线性回归 Cost Function 成本函数 Gradient Decent 梯度下降法 Normal Equations 正规方程 Probabilistic interpretation 概率论解释  Linear Regression 线性回归简单地$$h_\theta(x)=">
<meta property="og:image" content="http://angrycode.qiniudn.com/1346902099_4852.png">
<meta property="og:updated_time" content="2017-08-05T12:31:44.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习之线性回归">
<meta name="twitter:description" content="线性回归属于监督学习算法中的一种，也是最简单的一种算法。本文将讲述以下内容  Linear Regression 线性回归 Cost Function 成本函数 Gradient Decent 梯度下降法 Normal Equations 正规方程 Probabilistic interpretation 概率论解释  Linear Regression 线性回归简单地$$h_\theta(x)=">
<meta name="twitter:image" content="http://angrycode.qiniudn.com/1346902099_4852.png">
  
    <link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
</head>

  
    
      <body>
    
  
      <div id="container" class="container">
        <article id="post-机器学习之线性回归" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav id="main-nav" class="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    
      <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
    
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习之线性回归
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p>线性回归属于<strong>监督学习</strong>算法中的一种，也是最简单的一种算法。本文将讲述以下内容</p>
<ol>
<li>Linear Regression 线性回归</li>
<li>Cost Function 成本函数</li>
<li>Gradient Decent 梯度下降法</li>
<li>Normal Equations 正规方程</li>
<li>Probabilistic interpretation 概率论解释</li>
</ol>
<h2 id="Linear-Regression-线性回归"><a href="#Linear-Regression-线性回归" class="headerlink" title="Linear Regression 线性回归"></a>Linear Regression 线性回归</h2><p>简单地<br>$$<br>h_\theta(x)=\theta_0+\theta_1x<br>$$<br>若有5个样本，令$x<em>0$=1，那么<br>$$<br>\begin{bmatrix}<br>h</em>\theta(x^{(1)})\<br>h<em>\theta(x^{(2)})\<br>h</em>\theta(x^{(3)})\<br>h<em>\theta(x^{(4)})\<br>h</em>\theta(x^{(5)})\<br>\end{bmatrix}=\begin{bmatrix}<br>1&amp;x^{(1)}\<br>1&amp;x^{(2)}\<br>1&amp;x^{(3)}\<br>1&amp;x^{(4)}\<br>1&amp;x^{(5)}\<br>\end{bmatrix}<br>\begin{bmatrix}<br>\theta_0\<br>\theta<em>1<br>\end{bmatrix}<br>$$<br>若有2个变量<br>$$<br>h</em>\theta(x)=\theta_0+\theta_1x_1+\theta_2x<em>2<br>$$<br>那么<br>$$<br>\begin{bmatrix}<br>h</em>\theta(x^{(1)})\<br>h<em>\theta(x^{(2)})\<br>h</em>\theta(x^{(3)})\<br>h<em>\theta(x^{(4)})\<br>h</em>\theta(x^{(5)})\<br>\end{bmatrix}=\begin{bmatrix}<br>1&amp;x_1^{(1)}&amp;x_2^{(1)}\<br>1&amp;x_1^{(2)}&amp;x_2^{(2)}\<br>1&amp;x_1^{(3)}&amp;x_2^{(3)}\<br>1&amp;x_1^{(4)}&amp;x_2^{(4)}\<br>1&amp;x_1^{(5)}&amp;x_2^{(5)}\<br>\end{bmatrix}<br>\begin{bmatrix}<br>\theta_0\<br>\theta<em>1<br>\end{bmatrix}<br>$$<br>一般地<br>$$<br>h</em>\theta(x) = \theta_0 + \theta_1x_1+…+\theta_nx<em>n<br>$$<br>写成向量的形式<br>$$<br>h</em>\theta(x)=\sum_i^m\theta_ix_i=\theta^Tx<br>$$</p>
<p>其中$\theta$是权重，$x<em>i$是feature，$h</em>\theta(x)$是hypothesis。</p>
<p>对于有 m 个样本，更一般地为<br>$$<br>h_\theta(x^{(i)})=\theta^Tx^{(i)}<br>$$</p>
<h3 id="Cost-Function-成本函数"><a href="#Cost-Function-成本函数" class="headerlink" title="Cost Function 成本函数"></a>Cost Function 成本函数</h3><p>成本函数定义为<br>$$<br>J(\theta) = \frac 1 2 \sum<em>{i=1}^m (h</em>\theta(x^{(i)})-y^{(i)})^2<br>$$<br>其中 y 是真实值，而 h(x) 为机器预测值，i 表示第几个样本，前面的 1/2 是为了后面的计算方便引用的，当 y 和 h 的差的平方和为最小时，这样的直线就是我们要找的最佳拟合直线。</p>
<p>那如何找到这样$(\theta)$ 呢？这就用到了<strong>Gradient Descent</strong></p>
<h3 id="Gradient-Descent-梯度下降法"><a href="#Gradient-Descent-梯度下降法" class="headerlink" title="Gradient Descent 梯度下降法"></a>Gradient Descent 梯度下降法</h3><p>假设只有单变量的线性函数，那么 $J(\theta)$ 可以看出是一个碗状的面，如</p>
<p><img src="http://angrycode.qiniudn.com/1346902099_4852.png" alt=""></p>
<p>我们使用一些迭代公式进行搜索，找到最小值<br>$$<br>\theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta_j} J(\theta)<br>$$<br>其中 :=表示赋值符号，=表示比较两个值是否相等，$\alpha$ 是学习率。我们需要对 $\theta_j$ 求偏导</p>
<p>为了简单起见，我们考虑只有一个样本的情况，这样可以把<strong>累加符号</strong>去掉<br>$$<br>\begin{equation}\begin{split}\frac{\partial J(\theta)} {\partial \theta_j}&amp;=\frac{\partial } {\partial \theta<em>j}\frac 1 2 (h</em>\theta(x)-y)^2\<br>&amp;=2 \cdot\frac 1 2 (h_\theta(x)-y)\frac {\partial} {\partial \theta<em>j}(h</em>\theta(x)-y)\<br>&amp;= (h_\theta(x)-y)\frac {\partial} {\partial \theta_j}(\sum_i^n\theta_ix<em>i-y)\<br>&amp;= (h</em>\theta(x)-y)x_j\end{split}\end{equation}<br>$$<br>于是，当只有一个样本的情况时<br>$$<br>\theta_j:=\theta<em>j+\alpha(y^{(i)}-h</em>\theta(x^{(i)}))x_j<br>$$<br>推广到 n 个样本<br>$$<br>\theta_j:=\theta_j+\alpha\sum<em>i^n(y^{(i)}-h</em>\theta(x^{(i)}))x_j<br>$$</p>
<h3 id="Normal-Equations"><a href="#Normal-Equations" class="headerlink" title="Normal Equations"></a>Normal Equations</h3><p>假设X 是一个 m x n 的矩阵，那么可以<br>$$<br>X=\begin{bmatrix}-{(x^{(1)})}^T-\-{(x^{(2)})}^T-\\vdots\-{(x^{(m)})}^T-\end{bmatrix}<br>$$<br>$\vec y$ 是一个 m 维向量<br>$$<br>\vec y = \begin{bmatrix}y^{(1)}\y^{(2)}\\vdots\y^{(m)}\end{bmatrix}<br>$$<br>由于<br>$$<br>h_\theta(x^{(i)})=(x^{(i)})^T\theta<br>$$</p>
<p>$$<br>\begin{equation}\begin{split}X\theta-\vec y&amp;=\begin{bmatrix}(x^{(1)^T})\(x^{(2)^T})\\vdots\(x^{(m)^T})\end{bmatrix}-\begin{bmatrix}y^{(1)}\y^{(2)}\\vdots\y^{(m)}\end{bmatrix}\<br>&amp;=\begin{bmatrix}(x^{(1)})^T-y^{(1)}\x^{(2)})^T-y^{(2)}\\vdots\x^{(m)})^T-y^{(m)}\end{bmatrix}\end{split}\end{equation}<br>$$</p>
<p>又如果 z 是一个向量，那么有<br>$$<br>z^Tz=\sum_iz_i^2<br>$$<br>所以<br>$$<br>\begin{equation}\begin{split}\frac 1 2(X\theta-\vec y)^T(X\theta-\vec y^)<br>&amp;=\frac 1 2 \sum<em>i^m(h</em>\theta(x^{i})-y^{(i)})^2\<br>&amp;=J(\theta)\end{split}\end{equation}<br>$$</p>
<p>那么要求 $J(\theta)$ 的最小值，那么需要对其求导，求得极值点。</p>
<p>这时候会用到对矩阵的求导公式<br>$$<br>\begin{equation}\begin{split}\nabla<em>\theta J(\theta)&amp;=\nabla</em>\theta\frac 1 2(X\theta-\vec y)^T(X\theta-\vec y^)\<br>&amp;=\frac 1 2 \nabla<em>\theta(\theta^T X^T X\theta-\theta^TX^T\vec y-{\vec y}^TX\theta+{\vec y}^T\vec y)\<br>&amp;=\frac 1 2 \nabla</em>\theta tr(\theta^T X^T X\theta-\theta^TX^T\vec y-{\vec y}^TX\theta+{\vec y}^T\vec y)\<br>&amp;=\frac 1 2\nabla<em>\theta (tr\theta^TX^TX\theta-tr\theta^TX^T\vec y-tr\vec y^TX\theta)\<br>&amp;=\frac 1 2\nabla</em>\theta (tr\theta^TX^TX\theta-tr(\vec y^T(X\theta))^T-tr\vec y^TX\theta)\<br>&amp;=\frac 1 2\nabla_\theta (tr\theta^TX^TX\theta-2tr\vec y^TX\theta)\<br>&amp;=\frac 1 2(X^TX\theta+X^TX\theta-2X^T\vec y)\<br>&amp;=X^TX\theta-X^T\vec y\end{split}\end{equation}<br>$$<br>第3步用到<br>$$<br>trR=R,\text {$R$是实数}<br>$$<br>第5步用到<br>$$<br> trA=trA^T<br>$$<br>最后用到以下公式<br>$$<br>\nabla_AtrAB=\nabla_AtrBA=B^T\tag 1<br>$$</p>
<p>$$<br>\nabla_{A^T}f(A)=(\nabla_Af(A))^T\tag 2<br>$$</p>
<p>$$<br>\nabla_AtrABA^TC=CAB+C^TAB^T\tag 3<br>$$</p>
<p>由公式(2)(3)<br>$$<br>\nabla<em>{A^T}trABA^TC=B^TA^TC^T+BA^TC\tag 4<br>$$<br>若 C=I，即 C 为单位矩阵，那么<br>$$<br>\nabla</em>{A^T}ABA^T=B^TA^T+BA^T\tag 5<br>$$<br> 故要求得 $J(\theta)$ 最小值，就令导数等于0<br>$$<br>X^TX\theta=X^T\vec y<br>$$<br>于是就得到所谓的 <strong>normal equations</strong></p>
<p>$$<br>\theta=(X^TX)^{-1}X^T\vec y<br>$$</p>
<h3 id="Probabilistic-interpretation-概率论解释"><a href="#Probabilistic-interpretation-概率论解释" class="headerlink" title="Probabilistic interpretation 概率论解释"></a>Probabilistic interpretation 概率论解释</h3><p>从上文可以知道成本函数为<br>$$<br>J(\theta) = \frac 1 2 \sum<em>{i=1}^m (h</em>\theta(x^{(i)})-y^{(i)})^2<br>$$<br>那么为什么会这样定义呢？下文我们来从概率论的角度来说明。</p>
<p>假设目标变量 y 与输入变量存在以下关系<br>$$<br>y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}<br>$$<br>$\epsilon^{(i)}$ 表示误差项，例如样本的测量误差等因素。我们再假设这个误差是<strong>独立同分布</strong>的，即它是符合<strong>高斯分布或标准正态分布</strong>，那么<br>$$<br>p(\epsilon^{(i)})=\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {( \epsilon^{(i)})^2}{2\sigma^2}\right)<br>$$<br>于是<br>$$<br>p(y^{(i)}|x^{(i)};\theta)=\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)<br>$$<br>$p(y^{(i)}|x^{(i)};\theta)$ 表示给定以$\theta$为参数的$x^{(i)} y^{(i)}$的分布。同时需要注意的时，中间的是分号不能写成逗号，因为$\theta$不是随机变量。</p>
<p>通常我们把$p(y^{(i)}|x^{(i)};\theta)$ 称之为 <strong>Likelihood Function（似然函数）</strong><br>$$<br>L(\theta)=L(\theta;X,\vec y)=p(\vec y|X;\theta)<br>$$<br>还可以写成<br>$$<br>\begin{equation}\begin{split}L(\theta)&amp;=\prod<em>{i=1}^mp(y^{(i)}|x^{(i)};\theta)\<br>&amp;=\prod</em>{i=1}^m\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\end{split}\end{equation}<br>$$</p>
<p>要求 $\theta$ 使得样本中的值出现的概率是最大的，于是要求似然函数的最大值。</p>
<p>把上面的式子的连乘运算转化成加和运算，可以化简计算<br>$$<br>\begin{equation}\begin{split}l(\theta)&amp;=logL(\theta)\<br>&amp;=log\prod<em>{i=1}^m\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>&amp;=\sum</em>{i=1}^mlog\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>&amp;=\sum<em>{i=1}^mlog\frac 1 {\sqrt{2\pi\sigma^2}}+\sum</em>{i=1}^m\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>&amp;=m\cdot log \frac 1 {\sqrt{2\pi\sigma^2}}-\frac 1 {\sigma^2}\cdot \frac 1 2\sum<em>{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2\end{split}\end{equation}<br>$$<br>于是，要求$L(\theta)$ 的最大值，就是要求以下式子的最小值<br>$$<br>\frac 1 2 \sum</em>{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2<br>$$<br>而这个就是我们的 <strong>Cost Function（成本函数）</strong>。</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2017/08/05/机器学习之线性回归/" class="article-date">
  <time datetime="2017-08-05T12:31:44.000Z" itemprop="datePublished">2017-08-05</time>
</a>

        </li>
        
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <span id="article-nav-newer" class="article-nav-link-wrap newer"></span>
  
  
    <a href="/2017/08/05/hello-world/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>


  
</article>




      </div>
      
    <footer id="footer" class="post-footer footer">
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>contact -&gt; wecodexyz@gmail.com</p>


      </div>
    </footer>

      

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->







    </div>
  </body>
</html>
