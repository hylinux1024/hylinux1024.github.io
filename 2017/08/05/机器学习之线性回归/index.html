<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习之线性回归 | Machine Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="线性回归属于监督学习算法中的一种，也是最简单的一种算法。本文将讲述以下内容  Linear Regression 线性回归 Cost Function 成本函数 Gradient Decent 梯度下降法 Normal Equations 正规方程 Probabilistic interpretation 概率论解释  Linear Regression 线性回归简单地$$h_\theta(x)=">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习之线性回归">
<meta property="og:url" content="https://wecodexyz.github.io/2017/08/05/机器学习之线性回归/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="线性回归属于监督学习算法中的一种，也是最简单的一种算法。本文将讲述以下内容  Linear Regression 线性回归 Cost Function 成本函数 Gradient Decent 梯度下降法 Normal Equations 正规方程 Probabilistic interpretation 概率论解释  Linear Regression 线性回归简单地$$h_\theta(x)=">
<meta property="og:image" content="http://angrycode.qiniudn.com/1346902099_4852.png">
<meta property="og:updated_time" content="2017-08-05T14:57:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习之线性回归">
<meta name="twitter:description" content="线性回归属于监督学习算法中的一种，也是最简单的一种算法。本文将讲述以下内容  Linear Regression 线性回归 Cost Function 成本函数 Gradient Decent 梯度下降法 Normal Equations 正规方程 Probabilistic interpretation 概率论解释  Linear Regression 线性回归简单地$$h_\theta(x)=">
<meta name="twitter:image" content="http://angrycode.qiniudn.com/1346902099_4852.png">
  
    <link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Machine Learning</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">wecodexyz&#39;s learing notes</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://wecodexyz.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-机器学习之线性回归" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/08/05/机器学习之线性回归/" class="article-date">
  <time datetime="2017-08-05T14:47:05.000Z" itemprop="datePublished">2017-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习之线性回归
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>线性回归属于<strong>监督学习</strong>算法中的一种，也是最简单的一种算法。本文将讲述以下内容</p>
<ol>
<li>Linear Regression 线性回归</li>
<li>Cost Function 成本函数</li>
<li>Gradient Decent 梯度下降法</li>
<li>Normal Equations 正规方程</li>
<li>Probabilistic interpretation 概率论解释</li>
</ol>
<h2 id="Linear-Regression-线性回归"><a href="#Linear-Regression-线性回归" class="headerlink" title="Linear Regression 线性回归"></a>Linear Regression 线性回归</h2><p>简单地<br>$$<br>h_\theta(x)=\theta_0+\theta_1x<br>$$</p>
<p>若有5个样本，令 $x_0$=1，那么</p>
<p>$$<br>\begin{bmatrix}h<em>\theta(x^{(1)})\<br>h</em>\theta(x^{(2)})\<br>h<em>\theta(x^{(3)})\<br>h</em>\theta(x^{(4)})\<br>h_\theta(x^{(5)})\<br>\end{bmatrix}=\begin{bmatrix}<br>1&amp;x^{(1)}\<br>1&amp;x^{(2)}\<br>1&amp;x^{(3)}\<br>1&amp;x^{(4)}\<br>1&amp;x^{(5)}\<br>\end{bmatrix}<br>\begin{bmatrix}<br>\theta_0\<br>\theta_1<br>\end{bmatrix}<br>$$</p>
<p>若有2个变量</p>
<p>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2<br>$$</p>
<p>那么</p>
<p>$$<br>\begin{bmatrix}<br>h<em>\theta(x^{(1)})\<br>h</em>\theta(x^{(2)})\<br>h<em>\theta(x^{(3)})\<br>h</em>\theta(x^{(4)})\<br>h_\theta(x^{(5)})\<br>\end{bmatrix}=\begin{bmatrix}<br>1&amp;x_1^{(1)}&amp;x_2^{(1)}\<br>1&amp;x_1^{(2)}&amp;x_2^{(2)}\<br>1&amp;x_1^{(3)}&amp;x_2^{(3)}\<br>1&amp;x_1^{(4)}&amp;x_2^{(4)}\<br>1&amp;x_1^{(5)}&amp;x_2^{(5)}\<br>\end{bmatrix}<br>\begin{bmatrix}<br>\theta_0\<br>\theta_1<br>\end{bmatrix}<br>$$</p>
<p>一般地</p>
<p>$$<br>h_\theta(x) = \theta_0 + \theta_1x_1+…+\theta_nx_n<br>$$</p>
<p>写成向量的形式</p>
<p>$$<br>h_\theta(x)=\sum_i^m\theta_ix_i=\theta^Tx<br>$$</p>
<p>其中$\theta$是权重，$x<em>i$是feature，$h</em>\theta(x)$是hypothesis。<br>对于有 m 个样本，更一般地为</p>
<p>$$<br>h_\theta(x^{(i)})=\theta^Tx^{(i)}<br>$$</p>
<h3 id="Cost-Function-成本函数"><a href="#Cost-Function-成本函数" class="headerlink" title="Cost Function 成本函数"></a>Cost Function 成本函数</h3><p>成本函数定义为</p>
<p>$$<br>J(\theta) = \frac 1 2 \sum<em>{i=1}^m (h</em>\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
<p>其中 y 是真实值，而 h(x) 为机器预测值，i 表示第几个样本，前面的 1/2 是为了后面的计算方便引用的，当 y 和 h 的差的平方和为最小时，这样的直线就是我们要找的最佳拟合直线。</p>
<p>那如何找到这样$(\theta)$ 呢？这就用到了<strong>Gradient Descent</strong></p>
<h3 id="Gradient-Descent-梯度下降法"><a href="#Gradient-Descent-梯度下降法" class="headerlink" title="Gradient Descent 梯度下降法"></a>Gradient Descent 梯度下降法</h3><p>假设只有单变量的线性函数，那么 $J(\theta)$ 可以看出是一个碗状的面，如</p>
<p><img src="http://angrycode.qiniudn.com/1346902099_4852.png" alt=""></p>
<p>我们使用一些迭代公式进行搜索，找到最小值</p>
<p>$$<br>\theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta_j} J(\theta)<br>$$</p>
<p>其中 <strong>:=</strong>表示赋值符号，=表示比较两个值是否相等，$\alpha$ 是学习率。我们需要对 $\theta_j$ 求偏导</p>
<p>为了简单起见，我们考虑只有一个样本的情况，这样可以把<strong>累加符号</strong>去掉</p>
<p>$$<br>\begin{equation}\begin{split}\frac{\partial J(\theta)} {\partial \theta_j}&amp;=\frac{\partial } {\partial \theta<em>j}\frac 1 2 (h</em>\theta(x)-y)^2\<br>&amp;=2 \cdot\frac 1 2 (h_\theta(x)-y)\frac {\partial} {\partial \theta<em>j}(h</em>\theta(x)-y)\<br>&amp;= (h_\theta(x)-y)\frac {\partial} {\partial \theta_j}(\sum_i^n\theta_ix<em>i-y)\<br>&amp;= (h</em>\theta(x)-y)x_j\end{split}\end{equation}<br>$$</p>
<p>于是，当只有一个样本的情况时</p>
<p>$$<br>\theta_j:=\theta<em>j+\alpha(y^{(i)}-h</em>\theta(x^{(i)}))x_j<br>$$</p>
<p>推广到 n 个样本</p>
<p>$$<br>\theta_j:=\theta_j+\alpha\sum<em>i^n(y^{(i)}-h</em>\theta(x^{(i)}))x_j<br>$$</p>
<h3 id="Normal-Equations"><a href="#Normal-Equations" class="headerlink" title="Normal Equations"></a>Normal Equations</h3><p>假设X 是一个 m x n 的矩阵，那么可以</p>
<p>$$<br>X=\begin{bmatrix}-{(x^{(1)})}^T-\-{(x^{(2)})}^T-\\vdots\-{(x^{(m)})}^T-\end{bmatrix}<br>$$</p>
<p>$\vec y$ 是一个 m 维向量</p>
<p>$$<br>\vec y = \begin{bmatrix}y^{(1)}\y^{(2)}\\vdots\y^{(m)}\end{bmatrix}<br>$$</p>
<p>由于</p>
<p>$$<br>h_\theta(x^{(i)})=(x^{(i)})^T\theta<br>$$</p>
<p>$$<br>\begin{equation}\begin{split}X\theta-\vec y&amp;=\begin{bmatrix}(x^{(1)^T})\(x^{(2)^T})\\vdots\(x^{(m)^T})\end{bmatrix}-\begin{bmatrix}y^{(1)}\y^{(2)}\\vdots\y^{(m)}\end{bmatrix}\<br>&amp;=\begin{bmatrix}(x^{(1)})^T-y^{(1)}\x^{(2)})^T-y^{(2)}\\vdots\x^{(m)})^T-y^{(m)}\end{bmatrix}\end{split}\end{equation}<br>$$</p>
<p>又如果 z 是一个向量，那么有</p>
<p>$$<br>z^Tz=\sum_iz_i^2<br>$$</p>
<p>所以</p>
<p>$$<br>\begin{equation}\begin{split}\frac 1 2(X\theta-\vec y)^T(X\theta-\vec y^)<br>&amp;=\frac 1 2 \sum<em>i^m(h</em>\theta(x^{i})-y^{(i)})^2\<br>&amp;=J(\theta)\end{split}\end{equation}<br>$$</p>
<p>那么要求 $J(\theta)$ 的最小值，那么需要对其求导，求得极值点。</p>
<p>这时候会用到对矩阵的求导公式</p>
<p>$$<br>\begin{equation}\begin{split}\nabla<em>\theta J(\theta)&amp;=\nabla</em>\theta\frac 1 2(X\theta-\vec y)^T(X\theta-\vec y^)\<br>&amp;=\frac 1 2 \nabla<em>\theta(\theta^T X^T X\theta-\theta^TX^T\vec y-{\vec y}^TX\theta+{\vec y}^T\vec y)\<br>&amp;=\frac 1 2 \nabla</em>\theta tr(\theta^T X^T X\theta-\theta^TX^T\vec y-{\vec y}^TX\theta+{\vec y}^T\vec y)\<br>&amp;=\frac 1 2\nabla<em>\theta (tr\theta^TX^TX\theta-tr\theta^TX^T\vec y-tr\vec y^TX\theta)\<br>&amp;=\frac 1 2\nabla</em>\theta (tr\theta^TX^TX\theta-tr(\vec y^T(X\theta))^T-tr\vec y^TX\theta)\<br>&amp;=\frac 1 2\nabla_\theta (tr\theta^TX^TX\theta-2tr\vec y^TX\theta)\<br>&amp;=\frac 1 2(X^TX\theta+X^TX\theta-2X^T\vec y)\<br>&amp;=X^TX\theta-X^T\vec y\end{split}\end{equation}<br>$$</p>
<p>第3步用到</p>
<p>$$<br>trR=R,\text {$R$是实数}<br>$$</p>
<p>第5步用到</p>
<p>$$<br> trA=trA^T<br>$$</p>
<p>最后用到以下公式</p>
<p>$$<br>\nabla_AtrAB=\nabla_AtrBA=B^T\tag 1<br>$$</p>
<p>$$<br>\nabla_{A^T}f(A)=(\nabla_Af(A))^T\tag 2<br>$$</p>
<p>$$<br>\nabla_AtrABA^TC=CAB+C^TAB^T\tag 3<br>$$</p>
<p>由公式(2)(3)</p>
<p>$$<br>\nabla_{A^T}trABA^TC=B^TA^TC^T+BA^TC\tag 4<br>$$</p>
<p>若 C=I，即 C 为单位矩阵，那么</p>
<p>$$<br>\nabla_{A^T}ABA^T=B^TA^T+BA^T\tag 5<br>$$</p>
<p>故要求得 $J(\theta)$ 最小值，就令导数等于0</p>
<p>$$<br>X^TX\theta=X^T\vec y<br>$$</p>
<p>于是就得到所谓的 <strong>normal equations</strong></p>
<p>$$<br>\theta=(X^TX)^{-1}X^T\vec y<br>$$</p>
<h3 id="Probabilistic-interpretation-概率论解释"><a href="#Probabilistic-interpretation-概率论解释" class="headerlink" title="Probabilistic interpretation 概率论解释"></a>Probabilistic interpretation 概率论解释</h3><p>从上文可以知道成本函数为</p>
<p>$$<br>J(\theta) = \frac 1 2 \sum<em>{i=1}^m (h</em>\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
<p>那么为什么会这样定义呢？下文我们来从概率论的角度来说明。</p>
<p>假设目标变量 y 与输入变量存在以下关系</p>
<p>$$<br>y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}<br>$$</p>
<p>$\epsilon^{(i)}$ 表示误差项，例如样本的测量误差等因素。我们再假设这个误差是<strong>独立同分布</strong>的，即它是符合<strong>高斯分布或标准正态分布</strong>，那么</p>
<p>$$<br>p(\epsilon^{(i)})=\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {( \epsilon^{(i)})^2}{2\sigma^2}\right)<br>$$</p>
<p>于是</p>
<p>$$<br>p(y^{(i)}|x^{(i)};\theta)=\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)<br>$$</p>
<p>$p(y^{(i)}|x^{(i)};\theta)$ 表示给定以$\theta$为参数的$x^{(i)} y^{(i)}$的分布。同时需要注意的时，中间的是分号不能写成逗号，因为$\theta$不是随机变量。</p>
<p>通常我们把$p(y^{(i)}|x^{(i)};\theta)$ 称之为 <strong>Likelihood Function（似然函数）</strong></p>
<p>$$<br>L(\theta)=L(\theta;X,\vec y)=p(\vec y|X;\theta)<br>$$</p>
<p>还可以写成</p>
<p>$$<br>\begin{equation}\begin{split}L(\theta)&amp;=\prod<em>{i=1}^mp(y^{(i)}|x^{(i)};\theta)\<br>&amp;=\prod</em>{i=1}^m\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\end{split}\end{equation}<br>$$</p>
<p>要求 $\theta$ 使得样本中的值出现的概率是最大的，于是要求似然函数的最大值。</p>
<p>把上面的式子的连乘运算转化成加和运算，可以化简计算</p>
<p>$$<br>\begin{equation}\begin{split}l(\theta)&amp;=logL(\theta)\<br>&amp;=log\prod<em>{i=1}^m\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>&amp;=\sum</em>{i=1}^mlog\frac 1 {\sqrt {2\pi\sigma^2}}exp\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>&amp;=\sum<em>{i=1}^mlog\frac 1 {\sqrt{2\pi\sigma^2}}+\sum</em>{i=1}^m\left(-\frac {(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}\right)\<br>&amp;=m\cdot log \frac 1 {\sqrt{2\pi\sigma^2}}-\frac 1 {\sigma^2}\cdot \frac 1 2\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2\end{split}\end{equation}<br>$$</p>
<p>于是，要求$L(\theta)$ 的最大值，就是要求以下式子的最小值</p>
<p>$$<br>\frac 1 2 \sum<em>{i=1}^m (h</em>\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
<p>而这个就是我们的 <strong>Cost Function（成本函数）</strong>。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://wecodexyz.github.io/2017/08/05/机器学习之线性回归/" data-id="cj5zer4oh0002gsqgfx24dcv5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/08/05/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/08/05/机器学习之线性回归/">机器学习之线性回归</a>
          </li>
        
          <li>
            <a href="/2017/08/05/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2017/08/05/README/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Machine Learning<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>